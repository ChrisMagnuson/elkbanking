input {
    file {
        path => "/path/to/your/transactions.json"
        start_position => beginning
        sincedb_path => "/dev/null"
        codec => json {}
    }
}

filter {
    mutate {
        add_field => { "DateTimeOfTransaction" => "%{[DTPOSTED][0]}" }
        add_field => { "TransactionAmount" => "%{[TRNAMT][0]}" }
    }
    mutate {
        #If you have a timezone I haven't hit before you will need to add a line here to convert it from ex. [-4:EDT] to -04:00
        gsub => [ "DateTimeOfTransaction", "\[\-4:EDT\]", "-04:00" ]
        gsub => [ "DateTimeOfTransaction", "\[\-5:EST\]", "-05:00" ]
    }
    mutate {
        convert => { "TransactionAmount" => "float" }
    }

    ruby {
        code => "event['TransactionAmountAbsoluteValue'] = event['TransactionAmount'].abs"
    }
    #grock {
    #    match => { [DTPOSTED][0] => "\d{14}\.\d{3}\[[^\]]\]" }
    #}
    date { 
        #example 20151006000000[-4:EDT]
        #example after gsub 20151006000000-04:00
        match => [ "DateTimeOfTransaction", "yyyyMMddHHmmssZZ" ]
    }
}

output {
    #stdout { codec => rubydebug }
    stdout { codec => dots }
    elasticsearch {
        hosts => "localhost:9200"
        index => "logstash-username" #The logstash- prefix is neccessary until we define a custom elasticsearch index template of our own that gives us the .raw fields. For now we will piggyback off the built in template that is automatically used with indexes named logstash-
        document_id => "%{[FITID][0]}"
    }
}

